{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of original data\n",
    "==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importing needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# turn of warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pregnancies</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "Pregnancies                                                         \n",
       "6                148             72             35        0  33.6   \n",
       "1                 85             66             29        0  26.6   \n",
       "8                183             64              0        0  23.3   \n",
       "1                 89             66             23       94  28.1   \n",
       "0                137             40             35      168  43.1   \n",
       "\n",
       "             DiabetesPedigreeFunction  Age  Outcome  \n",
       "Pregnancies                                          \n",
       "6                               0.627   50        1  \n",
       "1                               0.351   31        0  \n",
       "8                               0.672   32        1  \n",
       "1                               0.167   21        0  \n",
       "0                               2.288   33        1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data= pandas.read_csv('../original_data/diabetes.csv', index_col=0);\n",
    "\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=data['Outcome']\n",
    "data=data.drop(['Outcome'],axis=1)\n",
    "## One-Hot Encoding\n",
    "enc2 = np.array(['Glucose','SkinThickness','Insulin','DiabetesPedigreeFunction'])\n",
    "\n",
    "##naive bayes data -> encoding all values because of the use of Bernouli NB\n",
    "nb_data=pandas.get_dummies(data,columns=list(data.columns))\n",
    "\n",
    "## saving the labels\n",
    "labels= list(data.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data in train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# splitting data for rf\n",
    "train_pred, test_pred, train_target, test_target = train_test_split(data, target, test_size=0.33,random_state=42)\n",
    "\n",
    "#splitting data for nb\n",
    "nb_train_pred, nb_test_pred, nb_train_target, nb_test_target = train_test_split(nb_data, target, test_size=0.33,random_state=42)\n",
    "\n",
    "display(list(set(nb_test_pred.columns)-set(nb_train_pred.columns)))\n",
    "## Transform everythin to np.array\n",
    "train_pred=np.array(train_pred)\n",
    "train_target=np.array(train_target)\n",
    "\n",
    "test_pred=np.array(test_pred)\n",
    "test_target=np.array(test_target)\n",
    "\n",
    "\n",
    "nb_test_pred=np.array(nb_test_pred)\n",
    "nb_train_pred=np.array(nb_train_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Train & predict with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier = classifier.fit(train_pred,train_target)\n",
    "\n",
    "rf_rediction = classifier.predict(test_pred)\n",
    "\n",
    "difference = rf_rediction - test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>0.289367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>0.183435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.163101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <td>0.125864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BloodPressure</th>\n",
       "      <td>0.094750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkinThickness</th>\n",
       "      <td>0.074484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insulin</th>\n",
       "      <td>0.068998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          importance\n",
       "Glucose                     0.289367\n",
       "BMI                         0.183435\n",
       "Age                         0.163101\n",
       "DiabetesPedigreeFunction    0.125864\n",
       "BloodPressure               0.094750\n",
       "SkinThickness               0.074484\n",
       "Insulin                     0.068998"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importances = pandas.DataFrame(classifier.feature_importances_,\n",
    "                                   index = labels,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "display(feature_importances.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance meassure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions:254\n",
      "Total number of wrong predictions:67\n",
      "Accuracy: 73.62204724409449 %\n"
     ]
    }
   ],
   "source": [
    "print('Total number of predictions:' + str(len(rf_rediction)))\n",
    "print('Total number of wrong predictions:' + str(np.count_nonzero(difference))) \n",
    "print('Accuracy: '+ str(((len(rf_rediction) - np.count_nonzero(difference))/len(rf_rediction))*100)+' %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Negative</th>\n",
       "      <td>134</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Positive</th>\n",
       "      <td>34</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted Negative  Predicted Positive\n",
       "Actual Negative                 134                  33\n",
       "Actual Positive                  34                  53"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the confusion matrix\n",
    "cm = metrics.confusion_matrix(rf_rediction, test_target)\n",
    "# Assigning columns names\n",
    "cm_df = pandas.DataFrame(cm, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80       168\n",
      "           1       0.61      0.62      0.61        86\n",
      "\n",
      "    accuracy                           0.74       254\n",
      "   macro avg       0.71      0.71      0.71       254\n",
      "weighted avg       0.74      0.74      0.74       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report( test_target,rf_rediction,target_names=['0','1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "Train & predict with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BernoulliNB()\n",
    "model = model.fit(nb_train_pred,nb_train_target)\n",
    "\n",
    "naive_prediction= model.predict(nb_test_pred)\n",
    "naive_difference = naive_prediction - nb_test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance meassure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions:254\n",
      "Total number of wrong predictions:82\n",
      "Accuracy: 67.71653543307087 %\n",
      "Accuracy: 0.6771653543307087\n"
     ]
    }
   ],
   "source": [
    "print('Total number of predictions:' + str(len(naive_prediction)))\n",
    "print('Total number of wrong predictions:' + str(np.count_nonzero(naive_difference))) \n",
    "print('Accuracy: '+ str(((len(naive_prediction) - np.count_nonzero(naive_difference))/len(naive_prediction))*100)+' %')\n",
    "print(\"Accuracy:\",metrics.accuracy_score(nb_test_target, naive_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Negative</th>\n",
       "      <td>154</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Positive</th>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted Negative  Predicted Positive\n",
       "Actual Negative                 154                  14\n",
       "Actual Positive                  68                  18"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the confusion matrix\n",
    "cm = metrics.confusion_matrix( test_target,naive_prediction)\n",
    "# Assigning columns names\n",
    "cm_df = pandas.DataFrame(cm, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.92      0.79       168\n",
      "           1       0.56      0.21      0.31        86\n",
      "\n",
      "    accuracy                           0.68       254\n",
      "   macro avg       0.63      0.56      0.55       254\n",
      "weighted avg       0.65      0.68      0.63       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report( test_target,naive_prediction,target_names=['0','1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN \n",
    "Train & predict with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(train_pred, train_target)\n",
    "\n",
    "knn_prediction=knn_model.predict(test_pred)\n",
    "knn_difference = knn_prediction - test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance meassure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions:254\n",
      "Total number of wrong predictions:79\n",
      "Accuracy: 68.89763779527559 %\n",
      "Accuracy: 0.6889763779527559\n"
     ]
    }
   ],
   "source": [
    "print('Total number of predictions:' + str(len(knn_prediction)))\n",
    "print('Total number of wrong predictions:' + str(np.count_nonzero(knn_difference))) \n",
    "print('Accuracy: '+ str(((len(knn_prediction) - np.count_nonzero(knn_difference))/len(knn_prediction))*100)+' %')\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_target, knn_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Negative</th>\n",
       "      <td>126</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Positive</th>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted Negative  Predicted Positive\n",
       "Actual Negative                 126                  42\n",
       "Actual Positive                  37                  49"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the confusion matrix\n",
    "cm = metrics.confusion_matrix(test_target,knn_prediction)\n",
    "# Assigning columns names\n",
    "cm_df = pandas.DataFrame(cm, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       168\n",
      "           1       0.54      0.57      0.55        86\n",
      "\n",
      "    accuracy                           0.69       254\n",
      "   macro avg       0.66      0.66      0.66       254\n",
      "weighted avg       0.69      0.69      0.69       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#metricy\n",
    "print(classification_report( test_target,knn_prediction,target_names=['0','1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "Calculating voting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_pred=[]\n",
    "i=0\n",
    "while i < len(prediction):\n",
    "    tmp=rf_rediction[i]+naive_prediction[i]+knn_prediction[i]\n",
    "    if tmp>1:\n",
    "        voting_pred.append(1)\n",
    "    else:\n",
    "        voting_pred.append(0)\n",
    "    i += 1\n",
    "voting_diff=voting_pred-test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance meassure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions:254\n",
      "Total number of wrong predictions:71\n",
      "Accuracy: 72.04724409448819 %\n",
      "Accuracy: 0.7204724409448819\n"
     ]
    }
   ],
   "source": [
    "print('Total number of predictions:' + str(len(voting_pred)))\n",
    "print('Total number of wrong predictions:' + str(np.count_nonzero(voting_diff))) \n",
    "print('Accuracy: '+ str(((len(knn_prediction) - np.count_nonzero(voting_diff))/len(voting_pred))*100)+' %')\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_target, voting_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Negative</th>\n",
       "      <td>137</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Positive</th>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted Negative  Predicted Positive\n",
       "Actual Negative                 137                  31\n",
       "Actual Positive                  40                  46"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the confusion matrix\n",
    "cm = metrics.confusion_matrix( test_target,voting_pred,)\n",
    "# Assigning columns names\n",
    "cm_df = pandas.DataFrame(cm, \n",
    "            columns = ['Predicted Negative', 'Predicted Positive'],\n",
    "            index = ['Actual Negative', 'Actual Positive'])\n",
    "# Showing the confusion matrix\n",
    "cm_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.79       168\n",
      "           1       0.60      0.53      0.56        86\n",
      "\n",
      "    accuracy                           0.72       254\n",
      "   macro avg       0.69      0.68      0.68       254\n",
      "weighted avg       0.71      0.72      0.72       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#metrics\n",
    "print(classification_report( test_target,voting_pred,target_names=['0','1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
